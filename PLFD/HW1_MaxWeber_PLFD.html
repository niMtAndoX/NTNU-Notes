<!DOCTYPE html>
<html>
<head>
<title>HW1_MaxWeber_PLFD.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="homework-1---predictive-learning-from-data"><strong>Homework 1 - Predictive Learning from Data</strong></h1>
<h2 id="risk-minimization--model-selection"><strong>Risk minimization &amp; model selection</strong></h2>
<h2 id="problem-27">Problem 2.7</h2>
<p>Goal of this problem was to estimate a k-NN classfier to predict the election results of each US-State using the obesity index (the percentage of people with a BMI over 30) as a single input variable. The underlying data are two csv files with the election result and the obesity index by state for the years 2000 and 2004. The following two graphs provide a brief overview of the data:</p>
<h4 id="head-of-the-csv-file">Head of the csv file</h4>
<table>
<thead>
<tr>
<th style="text-align:left">state</th>
<th style="text-align:right">obesity_index</th>
<th style="text-align:center">vote</th>
<th style="text-align:center">abbrv</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Alabama</td>
<td style="text-align:right">22.6</td>
<td style="text-align:center">R</td>
<td style="text-align:center">AL</td>
</tr>
<tr>
<td style="text-align:left">Alaska</td>
<td style="text-align:right">20.9</td>
<td style="text-align:center">R</td>
<td style="text-align:center">AK</td>
</tr>
<tr>
<td style="text-align:left">Arizona</td>
<td style="text-align:right">14.9</td>
<td style="text-align:center">R</td>
<td style="text-align:center">AZ</td>
</tr>
<tr>
<td style="text-align:left">Arkansas</td>
<td style="text-align:right">21.9</td>
<td style="text-align:center">R</td>
<td style="text-align:center">AR</td>
</tr>
<tr>
<td style="text-align:left">California</td>
<td style="text-align:right">18.7</td>
<td style="text-align:center">D</td>
<td style="text-align:center">CA</td>
</tr>
</tbody>
</table>
<p><img src="year_2000.png" alt="Figure 1 — Year 2000 Election results and obesity index with small jitter vor visibility"></p>
<p><img src="year_2004.png" alt="Figure 2 — Year 2004 Election results and obesity index with small jitter vor visibility"></p>
<p>First, the 2004 election results were used as training data, while the 2000 election results were used as test data. To choose the neighborhood size (k) for the k-NN classifier, I used leave-one-out cross-validation (LOOCV) on a dataset of (n = 50). In LOOCV, each fold trains on (n - 1 = 49) observations and tests on the single held-out point. Therefore, (k) must not exceed the training set size ((k \le n - 1)). Consequently, I tested (k) values from 1 to 49.</p>
<h4 id="results-of-the-loocv">Results of the LOOCV</h4>
<p><img src="LOOCV_results.png" alt="Figure 3 — LOOCV results"></p>
<table>
<thead>
<tr>
<th style="text-align:right">k</th>
<th style="text-align:right">LOOCV_accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">0.6078</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">0.6275</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">0.6863</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">0.7059</td>
</tr>
<tr>
<td style="text-align:right">5</td>
<td style="text-align:right">0.7059</td>
</tr>
<tr>
<td style="text-align:right">6</td>
<td style="text-align:right">0.7059</td>
</tr>
<tr>
<td style="text-align:right"><strong>7</strong></td>
<td style="text-align:right"><strong>0.7255</strong></td>
</tr>
<tr>
<td style="text-align:right">8</td>
<td style="text-align:right">0.7059</td>
</tr>
<tr>
<td style="text-align:right">9</td>
<td style="text-align:right">0.7255</td>
</tr>
<tr>
<td style="text-align:right">10</td>
<td style="text-align:right">0.6863</td>
</tr>
<tr>
<td style="text-align:right">11</td>
<td style="text-align:right">0.6275</td>
</tr>
<tr>
<td style="text-align:right">12</td>
<td style="text-align:right">0.6078</td>
</tr>
<tr>
<td style="text-align:right">13</td>
<td style="text-align:right">0.6471</td>
</tr>
<tr>
<td style="text-align:right">14</td>
<td style="text-align:right">0.6863</td>
</tr>
<tr>
<td style="text-align:right">15</td>
<td style="text-align:right">0.7059</td>
</tr>
<tr>
<td style="text-align:right">16</td>
<td style="text-align:right">0.7059</td>
</tr>
<tr>
<td style="text-align:right">17</td>
<td style="text-align:right">0.7059</td>
</tr>
<tr>
<td style="text-align:right">18</td>
<td style="text-align:right">0.6863</td>
</tr>
<tr>
<td style="text-align:right">19</td>
<td style="text-align:right">0.6667</td>
</tr>
<tr>
<td style="text-align:right">20</td>
<td style="text-align:right">0.7255</td>
</tr>
</tbody>
</table>
<p>The LOOCV results indicate that the value of (k) with the highest accuracy is (k = 7). With this (k), we can now train the k-NN classifier on the 2004 election results.</p>
<h3 id="predictions-and-and-metrics-of-the-k-nn-classifier">Predictions and and metrics of the k-NN classifier</h3>
<p><img src="k-NN_2000_predictions.png" alt="Figure 4 — year 2000 predictions"></p>
<table>
<thead>
<tr>
<th>Resampling Error</th>
<th>Resampling Accuracy</th>
<th>Best k</th>
<th>Test Error</th>
<th>Test Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>27.45%</td>
<td>72.55%</td>
<td>7</td>
<td>52.94%</td>
<td>47.06%</td>
</tr>
</tbody>
</table>
<h3 id="discussion">Discussion</h3>
<p>The model performs quite well on the resampled data. A resampling error of 27.45% means the accuracy during resampling was ~72.55%. Thus, on the training data the model looks fairly good. However, on the test data it performed very poorly. A test error of 52.94% means the model was correct only around 47% of the time, which, for a binary classification problem, is essentially as bad as guessing. The large gap between the resampling error and the test error suggests overfitting, meaning the model performs well on known training data but does not generalize well to new data. In practice, this suggests that the predictor (obesity index) alone might not be sufficient to predict vote reliably.</p>
<h2 id="problem-28">Problem 2.8</h2>
<p>Goal of this problem was to switch test and training data. So the year 2000 election results were used as training data and the year 2004 election results were used as test data.</p>
<h4 id="results-of-the-loocv">Results of the LOOCV</h4>
<p><img src="LOOCV_2.8_results.png" alt="Figure 5 — LOOCV results"></p>
<table>
<thead>
<tr>
<th style="text-align:right">k</th>
<th style="text-align:right">LOOCV_accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">0.6078</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">0.5490</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">0.5882</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">0.5882</td>
</tr>
<tr>
<td style="text-align:right">5</td>
<td style="text-align:right">0.5882</td>
</tr>
<tr>
<td style="text-align:right">6</td>
<td style="text-align:right">0.5294</td>
</tr>
<tr>
<td style="text-align:right">7</td>
<td style="text-align:right">0.5882</td>
</tr>
<tr>
<td style="text-align:right">8</td>
<td style="text-align:right">0.5098</td>
</tr>
<tr>
<td style="text-align:right">9</td>
<td style="text-align:right">0.5882</td>
</tr>
<tr>
<td style="text-align:right">10</td>
<td style="text-align:right">0.5686</td>
</tr>
<tr>
<td style="text-align:right">11</td>
<td style="text-align:right">0.5490</td>
</tr>
<tr>
<td style="text-align:right">12</td>
<td style="text-align:right">0.6078</td>
</tr>
<tr>
<td style="text-align:right">13</td>
<td style="text-align:right">0.6078</td>
</tr>
<tr>
<td style="text-align:right">14</td>
<td style="text-align:right">0.6078</td>
</tr>
<tr>
<td style="text-align:right">15</td>
<td style="text-align:right">0.6078</td>
</tr>
<tr>
<td style="text-align:right"><strong>16</strong></td>
<td style="text-align:right"><strong>0.6275</strong></td>
</tr>
<tr>
<td style="text-align:right">17</td>
<td style="text-align:right">0.6078</td>
</tr>
<tr>
<td style="text-align:right">18</td>
<td style="text-align:right">0.6078</td>
</tr>
<tr>
<td style="text-align:right">19</td>
<td style="text-align:right">0.5294</td>
</tr>
<tr>
<td style="text-align:right">20</td>
<td style="text-align:right">0.5882</td>
</tr>
</tbody>
</table>
<p>The LOOCV results indicate that the value of (k) with the highest accuracy is (k = 16). With this (k), we can now train the k-NN classifier on the 2000 election results.</p>
<h3 id="predictions-and-and-metrics-of-the-k-nn-classifier-with-switched-test-and-training-data">Predictions and and metrics of the k-NN classifier with switched test and training data</h3>
<p><img src="k-NN_2004_predictions.png" alt="Figure 6 — year 2000 predictions"></p>
<table>
<thead>
<tr>
<th>Resampling Error</th>
<th>Resampling Accuracy</th>
<th>Best k</th>
<th>Test Error</th>
<th>Test Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>37.25%</td>
<td>62.75%</td>
<td>16</td>
<td>37.25%</td>
<td>62.75%</td>
</tr>
</tbody>
</table>
<h3 id="changes">Changes</h3>
<p>We can see that, after swapping the training and test data, we get much better performance on new data but slightly worse performance on the already known data. The resampling error and the test error are equal, which suggests there is neither underfitting nor overfitting. The model generalizes well to new data.</p>
<h2 id="problem-211">Problem 2.11</h2>
<p><strong>Data Generating Process</strong></p>
<p>We observe (n=10) samples ((x_i, y_i)) with:</p>
<ul>
<li>(x_i \sim \mathrm{Unif}[0,1])</li>
<li>(y_i = x_i^2 + 0.1,x_i + \varepsilon_i), where (\varepsilon_i \sim \mathcal{N}(0,,0.25))</li>
</ul>
<h3 id="trigonometric-polynomial-estimators">Trigonometric polynomial estimators</h3>
<p>The goal was to find an optimal regression model using trigonometric polynomial estimators i.e.
( f_m(x,\mathbf{w}) = w_0 + \sum_{i=1}^{m} w_i \cos(2\pi i x) ), where (m+1) is the model complexity .</p>
<p>Since we want to use the analytic schwarz criterion, we need to use the following equation:</p>
<p>$$
r(p,n) = 1 + \frac{p}{1-p},\ln n
$$</p>
<p>Now we are able to determine which is the optimal model using the analytic schwarz criterion:</p>
<table>
<thead>
<tr>
<th style="text-align:right">m</th>
<th style="text-align:right">k</th>
<th style="text-align:right">RSS</th>
<th style="text-align:right">R_emp</th>
<th style="text-align:right">r(p,n)</th>
<th style="text-align:right">Schwarz_risk</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">4.04883</td>
<td style="text-align:right">0.404883</td>
<td style="text-align:right">1.25584</td>
<td style="text-align:right">0.508469</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">2</td>
<td style="text-align:right">1.04740</td>
<td style="text-align:right">0.104740</td>
<td style="text-align:right">1.57565</td>
<td style="text-align:right">0.165033</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">3</td>
<td style="text-align:right">0.999152</td>
<td style="text-align:right">0.0999152</td>
<td style="text-align:right">1.98682</td>
<td style="text-align:right">0.198514</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">4</td>
<td style="text-align:right">0.875019</td>
<td style="text-align:right">0.0875019</td>
<td style="text-align:right">2.53506</td>
<td style="text-align:right">0.221822</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">5</td>
<td style="text-align:right">0.869353</td>
<td style="text-align:right">0.0869353</td>
<td style="text-align:right">3.30259</td>
<td style="text-align:right">0.287111</td>
</tr>
<tr>
<td style="text-align:right">5</td>
<td style="text-align:right">6</td>
<td style="text-align:right">0.827483</td>
<td style="text-align:right">0.0827483</td>
<td style="text-align:right">4.45388</td>
<td style="text-align:right">0.368551</td>
</tr>
<tr>
<td style="text-align:right">6</td>
<td style="text-align:right">7</td>
<td style="text-align:right">0.704326</td>
<td style="text-align:right">0.0704326</td>
<td style="text-align:right">6.37270</td>
<td style="text-align:right">0.448846</td>
</tr>
<tr>
<td style="text-align:right">7</td>
<td style="text-align:right">8</td>
<td style="text-align:right">0.632242</td>
<td style="text-align:right">0.0632242</td>
<td style="text-align:right">10.21030</td>
<td style="text-align:right">0.645540</td>
</tr>
</tbody>
</table>
<p><strong>Best degree by analytic Schwarz:</strong>   m = 0</p>
<h3 id="interpretation">Interpretation</h3>
<p>As (m) increases, the training error (R_{\text{emp}}) decreases slightly, but the penalty (r(p,n)) grows quickly—especially as (k) approaches (n). In my table this makes the penalized score smallest at (m=0) ((k=1)), i.e., the intercept-only model (a flat line). Larger (m) fit the data slightly better in-sample (lower RSS) but are heavily penalized, so their Schwarz risk is higher. Thus, by the analytic Schwarz criterion, the simplest model is preferred.</p>
<h3 id="visualization-of-the-results">Visualization of the results</h3>
<p><img src="trig_best_m.png" alt="Figure 7 — schwarz vs m"></p>
<p><img src="trig_regression.png" alt="Figure 8 — trigonometric regression"></p>
<p>Since the data is non periodic and monotone increasing a trigonometric estimator using cosines will not be able to estimate the data well without being overly complex.</p>
<h2 id="211-b">2.11 b)</h2>
<h3 id="algebraic-polynomial-estimators">Algebraic polynomial estimators</h3>
<p>The goal was to find an optimal regression model now using algebraic polynomial estimators i.e.
$$
f_m(x)=w_0+w_1x+w_2x^2+\cdots+w_mx^m
$$</p>
<p>Now we can use these to determine the optimal model:</p>
<table>
<thead>
<tr>
<th style="text-align:right">m</th>
<th style="text-align:right">k</th>
<th style="text-align:right">RSS</th>
<th style="text-align:right">R_emp</th>
<th style="text-align:right">r(p,n)</th>
<th style="text-align:right">Schwarz_risk</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">4.04883</td>
<td style="text-align:right">0.404883</td>
<td style="text-align:right">1.25584</td>
<td style="text-align:right">0.508469</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">2</td>
<td style="text-align:right">1.04740</td>
<td style="text-align:right">0.104740</td>
<td style="text-align:right">1.57565</td>
<td style="text-align:right">0.165033</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">3</td>
<td style="text-align:right">0.999152</td>
<td style="text-align:right">0.0999152</td>
<td style="text-align:right">1.98682</td>
<td style="text-align:right">0.198514</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">4</td>
<td style="text-align:right">0.875019</td>
<td style="text-align:right">0.0875019</td>
<td style="text-align:right">2.53506</td>
<td style="text-align:right">0.221822</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">5</td>
<td style="text-align:right">0.869353</td>
<td style="text-align:right">0.0869353</td>
<td style="text-align:right">3.30259</td>
<td style="text-align:right">0.287111</td>
</tr>
<tr>
<td style="text-align:right">5</td>
<td style="text-align:right">6</td>
<td style="text-align:right">0.827483</td>
<td style="text-align:right">0.0827483</td>
<td style="text-align:right">4.45388</td>
<td style="text-align:right">0.368551</td>
</tr>
<tr>
<td style="text-align:right">6</td>
<td style="text-align:right">7</td>
<td style="text-align:right">0.704326</td>
<td style="text-align:right">0.0704326</td>
<td style="text-align:right">6.37270</td>
<td style="text-align:right">0.448846</td>
</tr>
<tr>
<td style="text-align:right">7</td>
<td style="text-align:right">8</td>
<td style="text-align:right">0.632242</td>
<td style="text-align:right">0.0632242</td>
<td style="text-align:right">10.21030</td>
<td style="text-align:right">0.645540</td>
</tr>
</tbody>
</table>
<p><strong>Best degree by analytic Schwarz:</strong> m = 1</p>
<h3 id="interpretation">Interpretation</h3>
<p>Jumping from (m=0) to (m=1) (adding a slope) reduces RSS by ~74% (4.05 → 1.05), so the penalized score drops to 0.165. From (m=1) to (m=2) and beyond, RSS improves only a little, but the penalty rises a lot, so the Schwarz_risk increases. The best trade-off is (m=1), i.e., a linear model (intercept + slope). Higher degrees overfit relative to their modest extra gain.</p>
<p><strong>Comparing to trigonometric models.</strong> Since the same criterion is used on the same data, you can compare the minima directly. The best polynomial ((m=1), 0.165) beats the best trigonometric model ((m=0), 0.508), so the polynomial family wins here.</p>
<h3 id="visualization-of-the-results">Visualization of the results</h3>
<p><img src="alg_best_m.png" alt="Figure 7 — schwarz vs m"></p>
<p><img src="alg_regression.png" alt="Figure 8 — trigonometric regression"></p>
<h2 id="problem-212">Problem 2.12</h2>
<p>The goal in this final problem was to compare the predictive accuracies of trigonometric and polynomial estimators using 5-fold cross-validation, after applying the analytic Schwarz criterion for model selection.</p>
<h3 id="5-fold-cross-validation-inner-schwarz-selection">5-Fold Cross-Validation (inner Schwarz selection)</h3>
<table>
<thead>
<tr>
<th>Model family</th>
<th style="text-align:center">CV-MSE</th>
<th>m* per fold</th>
</tr>
</thead>
<tbody>
<tr>
<td>Trigonometric (cos-only)</td>
<td style="text-align:center">0.8104</td>
<td>[5, 0, 0, 0, 0]</td>
</tr>
<tr>
<td>Polynomial (raw (x))</td>
<td style="text-align:center">0.4269</td>
<td>[2, 1, 1, 4, 1]</td>
</tr>
</tbody>
</table>
<p>Because lower cross-validated mean squared error (CV-MSE) indicates better out-of-sample prediction, the polynomial family clearly yields better results (0.43 vs. 0.81). The trigonometric family’s frequent selection of (m = 0) means that, on most training splits, a periodic structure isn’t necessary. The single instance with (m = 5) likely overfits that fold, which cross-validation penalizes with a higher average MSE. The polynomial results are stable around a linear trend (mostly (m = 1)). Higher degrees yield only small training gains that do not translate into lower validation error.</p>
<p>These results make sense because the data show a non-periodic, monotonic trend, which polynomials capture more naturally than cosine sums.</p>

</body>
</html>
